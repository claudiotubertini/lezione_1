# Capitolo XX Il web come editoria digitale

## 1. Cos'è l'editoria

### Aside

>Nota: Elenchiamo in ordine di difficoltà  

- [JavaScript First Steps](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/First_steps)  
- [The JavaScript Way](https://github.com/thejsway/thejsway)  
- [Eloquent JavaScript](https://eloquentjavascript.net/)  
- [Structure and Interpretation of Computer Programs](https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book.html)  

Leggere almeno una di queste introduzioni alla programmazione tenendo presente che la prima si rivolge a lettori completamente principianti, la terza è di qualche interesse anche per il lettore professionista, l'ultima costituisce un'introduzione generale utile a chi ha già provato a scrivere qualche programma. Tutti gli esempi di programmazione in questo saggio utilizzano Javascript, un linguaggio non particolarmente indicato per dei neofiti, al contrario dell'elegante e (spesso) molto chiaro Python. Ma JavaScript è un linguaggio di cui non si può fare a meno: sul web, inserito in un ePub o addirittura in un pdf, resta la soluzione da preferire. Per chi volesse approfondire i temi di cui si parla in questo capitolo è consigliabile apprendere almeno i rudimenti di Python. La bibliografia è sterminata, ma si può partire da questa pagina [**Beginners Guide**](https://wiki.python.org/moin/BeginnersGuide). L'ultimo volume citato all'inizio usa invece LISP, un linguaggio che non potrà non piacere ai lettori umanisti digitali.

Come emerge dai capitoli precedenti la produzione editoriale digitale può significativamente aiutare la formazione, l'interpretazione e la disseminazione della conoscenza nelle discipline umanistiche. La diffusione e la facile consultazione delle banche dati, il data mining, la costante e continua comunicazione con i colleghi o con i lettori modifica l'idea stessa di pubblicazione [Riva, 2017](https://claudiotubertini.github.io/ediotria/53-1-234-3-10-20180310.pdf).
Ora passeremo in rassegna, cercando di capirne alcuni meccanismi di funzionamento, gli aspetti tecnici dei media digitali destinati all'editoria e contemporaneamente cercando anche di acquisire qualche nozione di programmazione che ci eviti di essere utilizzatori _naive_. Guarderemo soprattutto al web, creato per comunicare e distribuire documenti [Vedi la prima pagina web pubblicata dal CERN](http://info.cern.ch/hypertext/WWW/TheProject.html), trasformato oggi in qualcosa di più complesso e diversificato, dove però i testi conservano comunque un ruolo rilevante. Così facendo emergerà una descrizione generale di cosa possa e debba essere una pubblicazione digitale, una pubblicazione che sia anche disponibile offline, sia condivisibile e facilmente trasmissibile, conservabile per un tempo teoricamente indefinito [publishing@W3C](https://www.w3.org/publishing/). Queste sono le caratteristiche che distinguono l'editoria digitale dai prodotti web, prodotti, questi ultimi, che per il resto appaiono sostanzialmente indistinguibili. Insomma per parlare di editoria guarderemo al web, anche se terremo ferme nel confronto alcune caratteristiche del libro tradizionale, essere accessibile offline, condivisibile e conservabile, un prodotto straordinariamente efficiente e di grande successo per poter essere abbandonato a cuor leggero.

È necessaria una qualche delucidazione terminologica. Quando si parla di editoria digitale il primo riferimento è all'organizzazione di produzione e distribuzione di pdf ed ePub. Ovvero a quei file che i lettori possono scambiarsi o acquistare su una qualunque libreria online. Noi considereremo editoriali invece tutte le tecnologie che consentono la trasmissione di contenuti "librari", contenuti cioè sufficientemente complessi tanto da richiedere l'intervento di un autore professionista, tecnologie sottoinsieme di un campo più vasto noto sotto il nome di tecnologie per la produzione di applicazioni web.

Ciò che viene comunemente chiamato e-book è costituito da un file PDF o un ePub, un gruppo di file HTML raccolti secondo regole convenzionali stabilite di comune accordo fra i principali produttori del settore. Per le specifiche tecniche si può leggere utilmente [EPUB 3.2](https://www.w3.org/publishing/epub32/epub-spec.html).
Dopo questa breve introduzione, dedicata a ciò che ci si aspetta da un saggio dedicato all'editoria digitale, ci guarderemo attorno iniziando dal web, e dalle pagine html che troviamo dopo una ricerca. Vedremo come possono comunicare dati e informazioni attraverso una semantica condivisa e ci impratichiremo un poco di alcune tecniche importanti, come le espressioni regolari o l'XPath. Nelle pagine successive vedremo come utilizzare dati per produrre contenuti interattivi (programmazione lato utente), e vedremo rapidamente anche come quei dati possano essere conservati e riutilizzati (programmazione lato server). Nel paragrafo successivo passeremo da quello che in effetti sono le pagine web a come i contenuti possono essere trasmessi efficacemente attraverso alcune tecnologie legate ai linguaggi di markup (le tecnologie XML) e anche come si può generare un file PDF, che resta uno strumento principe di comunicazione (insieme all'HTML).

Questo saggio ha a volte l'aspetto del tutorial, perché la lettura di questi temi ha bisogno di scrittura di codice e dell'esecuzione personale, da parte del lettore, dei programmi allegati, a volte anche complessi per un neofita (tutti gli esempi sono scaricabili da una account [github](https://claudiotubertini.github.io/editoria/). Ma ha anche un altro aspetto: cerca di essere un'introduzione ad alcuni concetti, propri dell'informatica, in qualche modo utili alla produzione editoriale umanistica, passati in rassegna in modo generale e sintetico, direi in modo saggistico, ovvero con qualche elemento di soggettività. Questo capitolo è anche uno dei tanti tentativi di risoluzione del problema didattico dell'insegnamento di nozioni interdisciplinari. La modalità che mi sembra più adeguata, dopo, lo confesso, alcuni anni di esperimenti alle spalle di ignari studenti, è quello di presentare una serie di attività pratiche, che richiedano sforzo di programmazione e letture integrative di varia origine, organizzate intorno a problemi specifici, a volte necessariamente, per ovvie ragioni di economia, descritte in termini vaghi. Questo dovrebbe consentire il consolidamento di alcune nozioni di base e allo stesso tempo spingere a letture ulteriori, per capire quello che è stato solo accennato, in un processo di costante ricerca che si spera proficuo sia per chi legge sia per chi scrive.  

## 2. Il lettore automatico

(Nota bibliografica - Una prima versione di questo paragrafo è stata pubblicata in _Il libro digitale. La parola agli editori_, a cura di Maria Villano, Biblioteca Clueb, 2020)

Per comprendere quale sia la struttura di una pagina web assumeremo un punto di vista (apparentemente) inusuale, quello di un lettore automatico, ad esempio lo spider di Google o di un simile motore di ricerca.
Il flusso di dati che aziende, istituzioni di ricerca e utenti si scambiano è imponente. Buona parte di questo flusso è in genere inaccessibile, conservato all’interno di _repositories_ privati, ma gli scambi con il web sono costanti e, presumiamo, importanti. L’attività di raccolta dati è così rilevante che oltre ai motori generalisti, come [Google](https://about.google/intl/en-US/), [DuckDuckGo](https://duckduckgo.com/?va=z&t=hc) o il nuovo e interessante, [cliqz](https://cliqz.com/en/about) esistono anche motori di ricerca specializzati: ad esempio [WolframAlpha](https://www.wolframalpha.com/), [Mendeley](https://www.mendeley.com/?interaction_required=true) e [Zotero](https://www.zotero.org/).
I motori di ricerca hanno fondamentalmente due compiti:

1. raccogliere dati significativi che illustrano il contenuto delle pagine web;
2. consentire ricerche per recuperare questi dati.
In queste pagine guarderemo ad alcuni aspetti della prima attività, rilevanti per l’editoria. La seconda attività è molto più tecnica e non avremo modo di trattarla in questa sede.  

### Che cos’è uno spider

Per raccogliere dati dal web si usano programmi chiamati _spider_ o _crawler_  (sono sinonimi). Questi programmi passano da un sito all’altro memorizzando, per semplicità, solo alcuni dati.
Vediamo un esempio di spider. Come avete visto nelle introduzioni alla programmazione che avete letto all'inizio del capitolo, javascript può essere usato in un browser per interpretare i programmi inclusi nel file html che avete scaricato da un server, ma può essere anche usato "autonomamente" sul vostro computer attraverso node ed eseguito in una console, in maniera molto simile a quello che si fa con python. In questo paragrafo vedremo sempre esempi sia in python sia in javascript.
Create un file, noi lo chiameremo link_crawler.py e copiatevi le seguenti righe  

```python
from urllib.request import urlopen, urljoin
import re
def download_page(url):
    return urlopen(url).read().decode('utf-8')
def extract_links(page):
4   link_regex = re.compile('<a[^>]+href=["\'](.*?)["\']', re.IGNORECASE)
    return link_regex.findall(page)

if __name__ == '__main__':
1   target_url = 'https://clueb.it/'
2   clueb = download_page(target_url)
3   links = extract_links(clueb)
    for link in links:
        print(urljoin(target_url, link))
```

Eseguitelo in una console linux:

```bash
python3 link_crawler.py > output.csv
```

La versione in javascript, che potete rinominare in link_crawler.js è la seguente

```javascript
const fetch = require('isomorphic-fetch')
target_url = 'https://clueb.it/'
let download_page =
    async function(target_url) {
                const response = await fetch(target_url);
                const text = await response.text();
                return text.match(/<a[^>]+href=["\'](.*?)["\']/gi);
                // questa espressione regolare dovrebbe restituire il gruppo 
                // contenuto nelle parentesi '(.*?)' ma in questo caso, a 
                // causa del flag '/g' non accade ed è necessario un 
                // secondo "passaggio" come indicato nelle righe successive
              }
1,2 links = download_page(target_url)
3 for (let link of links){
4    let innerlink = link.match(/href=["\'](.*?)["\']/)
     console.log(target_url + innerlink);
     }
```  

Eseguite questo file in una console node:

```bash
node link_crawler.js
```

Questo programma, così come il precedente, accede alla pagina web [clueb.it](https://clueb.it/) (1), la scarica (2) e ne estrae i link (3). I link sono individuati dall’espressione regolare (4).
Per costruire un vero e proprio link crawler dovremmo memorizzare tutti i link (che sono indirizzi di pagine web) e poi passarli uno a uno alla ricerca degli altri link contenuti in quelle pagine: una sorta di ricerca ricorsiva. Naturalmente ci ricorderemo delle pagine già viste e non torneremo a visitarle se non dopo un certo periodo di tempo.  

![Procedura ricorsiva di un link crawler](/home/claudio/Dropbox/corso_2019/manuale/grafo2.png)  

Questo è ciò che fa sistematicamente Google. Ma raccogliere e rendere ricercabili i contenuti è solo una parte della sua attività, la principale, e molto più sofisticata, è eseguire ricerche tenendo conto della profilazione dell’utente.  

### Che cos’è il web scraping

Entriamo ora in una singola pagina web alla ricerca di quelli che possiamo chiamare “dati”. Vedremo poi di caratterizzare meglio cosa si intende per “dato” in questo contesto.
Ecco una pagina web, un po’ semplificata.  

``` html
<!DOCTYPE html>
<html>
    <head>
  <meta charset="UTF-8">
  <title> I Promessi Sposi</title>
   </head>
<body>
  <div itemscope="" itemtype="http://schema.org/Book" itemprop="mainEntity">
  <img itemprop="image" src="https://en.wikipedia.org/wiki/File:Francesco_Hayez_040.jpg"
    alt="Alessandro Manzoni"/>
    <span itemprop="name">I promessi sposi</span> —
    <link itemprop="url" href="https://it.wikipedia.org/wiki/I_promessi_sposi"/><br />
    di <a itemprop="author" href="https://it.wikipedia.org/wiki/Alessandro_Manzoni">
      Alessandro Manzoni</a>
   </div>
    <div itemtype="http://schema.org/Offer" itemscope="" itemprop="offers">
      <span itemprop="name">I promessi sposi</span><br />
      <meta itemprop="priceCurrency" content="EURO" />
      <span itemprop="price">€ 19.95</span>
      <link itemprop="availability" href="http://schema.org/InStock">In Stock
    </div>
</div>
</body>
</html>
```  

Quando digitiamo l’url di una pagina web nel nostro browser scarichiamo un file HTML come questo.
Oltre a tag strutturali come `div` o semantici come `img` ne vediamo altri che individuano e illustrano aspetti contenutistici: `itemtype`, `itemprop`.  `Itemprop` individua una proprietà, ad esempio il prezzo, di `itemtype`, l’offerta in vendita di una copia dei _Promessi Sposi_. Qualsiasi contenuto può essere rappresentato, graficamente, da HTML e CSS, ma le relazioni fra i contenuti restano spesso implicite. Un essere umano è in grado di comprendere che se un prezzo è associato a un libro probabilmente si tratta di un’offerta di vendita. Ma la lettura automatica ha bisogno che gli elementi rilevanti del contesto comunicativo siano resi espliciti: `itemtype` e i diversi `itemprop` ci dicono che abbiamo a che fare con un offerta di un prodotto dotato di prezzo e disponibilità. [Mozilla Web Docs](https://developer.mozilla.org/en-US/docs/Web/HTML/Microdata)  

Guardiamo ora un esempio di web scraping, preleveremo dati da una pagina web. Ricordiamo che il primo passo è stato individuare una serie di URL, ora entriamo nelle singole pagine alla ricerca delle informazioni che ci interessano.
Utilizziamo la libreria python lxml.
Prendiamo come esempio questa pagina di un sito di e-commerce, scelto in maniera abbastanza casuale, [e-commerce]('https://www.libreriauniversitaria.it/macroeconomia-prospettiva-europea-blanchard-olivier/libro/9788815265715')  

``` python
from lxml import html
from lxml import etree
import requests
1  link = 'https://www.libreriauniversitaria.it/macroeconomia-prospettiva-\
            europea-blanchard-olivier/libro/9788815265715'
2  response = requests.get(link)
   sourceCode = response.content
   html_elem = html.fromstring(sourceCode)
3  for e in html_elem.xpath('//ul[@class="dettagli-prodotto"]/li'):
      print(e.text_content())
```  

In node abbiamo invece (notare la recente libreria `xpath-html`)  

``` javascript
const fetch = require('isomorphic-fetch')
const xpath = require("xpath-html");
1 link = 'https://www.libreriauniversitaria.it/macroeconomia-prospettiva-\
          europea-blanchard-olivier/libro/9788815265715'

fetch(link)
2    .then(function(response) {
        //La pagina viene prima scaricata e poi trasformata in testo
        return response.text()
    })
    .then(function(html) {
3      var nodes = xpath.fromPageSource(html).findElements('//ul[@class="dettagli-prodotto"]/li');
      var mynodes = xpath.fromNode(nodes).findElements("//li/span");
      var res = mynodes.map(x => x.getText());
      console.log(res); // non fate caso ai molti warnings che vedrete, guardate solo al risultato
    })
    .catch(function(err) {
        console.log('Failed to fetch page: ', err);
    });

```  

1 è l’URL di una pagina che mette in vendita un noto testo di macroeconomia; in 2 viene scaricata la pagina, in 3 attraverso un comando Xpath vengono scaricati i dati che ci interessano. Il comando Xpath è basato su un’analisi preliminare della pagina. Il programmatore ha visto che i dettagli del prodotto erano contenuti in una lista(‘//ul/li’), caratterizzata da una classe denominata “dettagli-prodotto”.
Per individuare gli elementi che cerchiamo potremmo usare anche i css selectors. Utilizziamo il file HTML già scaricato:  

```python
html_elem = html.fromstring('''<!DOCTYPE html> <html> <head>…</div></body></html>''')
from lxml.cssselect import CSSSelector
titolo = [ e.text_content() for e in html_elem.cssselect("h1[itemprop=name]")]
```  

In javascript è necessario aggiungere un'altra library dedicata al DOM  

```javascript
const jsdom = require("jsdom");
const { JSDOM } = jsdom;
fetch(link).then(function(response) {
        return response.text()
    }).then(function(html) {
        var dom = new JSDOM(html);
        var docArticle = dom.window.document.querySelector('ul.dettagli-prodotto li span').textContent;
        var mydocArticle = dom.window.document.querySelector('h1[itemprop]').textContent;
        console.log(docArticle);
        console.log(mydocArticle);
    }).catch(function(err) {
        console.log('Failed to fetch page: ', err);
    });
```  

Si potrebbe fare meglio ma per il momento è sufficiente, abbiamo ottenuto tutti i dati testuali che stavamo cercando e i tag `itemtype` e `itemprop` ci hanno aiutato in maniera essenziale. Sono tag conosciuti come “microdata” e sono un esempio di “dato strutturato”. Torneremo su questo argomento tra poco.
Per recuperare, e quindi successivamente riutilizzare, i dati che otteniamo con questo piccolo web scraper abbiamo sostanzialmente tre metodi. Il principale è  la ricerca con espressioni regolari. Lo abbiamo utilizzato nel link crawler. La ricerca di link era basata sul “pattern” che sapevamo costituire un link.
L’espressione  `<a[^>]+href=["\'](.*?)["\']` individua una serie di caratteri del tipo `<a href=”example.com”>` e restituisce solo l’url presente nell’attributo href. Le espressioni regolari, presenti in tutti i linguaggi di programmazione, sono forse lo strumento più potente per individuare sequenze fisse di caratteri in testi privi di tag. Gli altri metodi sono i selettori CSS e l’XPath. Entrambi richiedono, diversamente dalle espressioni regolari, che il testo da analizzare abbia un “markup” predefinito. Una riga di HTML
<a title=”un link” href=”example.com”> Un link</a>
può essere analizzata con il selettore CSS `a attr(href)`.  
L’XPath è un vero e proprio linguaggio dichiarativo e trova applicazione anche nei testi in XML, può contenere funzioni, come `contains`: `html_elem.xpath('//a[contains(text(), “link”)]/@href)`; e variabili: `html_elem.xpath('//a[contains(text(), $var)]/@href', var='link')`.  

In questo caso i tre metodi sono sostanzialmente equivalenti ma se il testo non contiene tag bisognerà utilizzare le espressioni regolari, in un testo taggato se la ricerca è complessa probabilmente si dovrà utilizzare l’XPath, in alternativa il CSS selector è in genere il più facile da usare.
Abbiamo tralasciato il caso in cui il risultato del web scraping sia un file JSON invece che HTML. In questo caso si dovrà ricorrere a scorciatoie utilizzando svariati metodi di programmazione anche se esiste un linguaggio di query simile all’XPath, conosciuto come [Jmspath](http://jmespath.org/).

Ricapitoliamo nuovamente la strada che abbiamo percorso. Per esplorare i contenuti diffusi in rete siamo partiti da un link crawler per passare poi all’analisi delle singole pagine web. Abbiamo visto diversi modi di analizzarne il contenuto con CSS, Xpath ed espressioni regolari. I microdata forniscono informazioni che migliorano l’efficienza della ricerca.  

### I dati strutturati

Siamo ora in grado di accedere a una grande quantità di dati, ma le relazioni fra i dati, in un qualche modo ciò che possiamo considerare il significato dei dati, rimane di difficile accesso. È  necessario l’intervento di un operatore umano che formuli quelle query che abbiamo visto parlando di CCS selectors, Xpath o espressioni regolari.  I crawler, come quello di Google, hanno invece bisogno di individuare facilmente i metadati di una pagina.  A tal fine si usano i “dati strutturati”. Ne esistono varie formulazioni, diverse nella sintassi ma sostanzialmente equivalenti per funzione. Oltre ai microdati, che abbiamo già visto, esistono i formati JSON-LD e RDFa. Per una descrizione approfondita si rimanda ai testi citati nelle note bibliografiche.
Tradizionalmente i metadati, il tag `title` e i diversi `meta` trovano posto all’interno del tag `head` nella pagina HTML. Sia i microdati che RDFa consentono di inserire metadati nel corpo della pagina, JSON-LD anche in un file a parte.
Per estrarre i metadati possiamo usare, in python, la libreria `extruct`, in javascript il modulo `microdata-node`. Prendiamo nuovamente la pagina della libreria online che abbiamo già studiato  

``` python
import extruct
import requests
import pprint
from w3lib.html import get_base_url
1 r = requests.get('https://www.libreriauniversitaria.it/macroeconomia-\
        prospettiva-europea-blanchard-olivier/libro/9788815265715')
2 base_url = get_base_url(r.text, r.url)
3 data = extruct.extract(r.content, base_url=base_url)
  pp = pprint.PrettyPrinter(indent=2)
pp.pprint(data)
```  

Nella riga 1 la library `requests` scarica il file html, la riga 2 controlla le url e gli eveentuali riferimenti relativi, la riga 3 estrae tutti i dati e `pprint` li stampa in modo da poterli leggere.  

``` javascript
const fetch = require('isomorphic-fetch')
var microdata = require('microdata-node');
link = 'https://www.libreriauniversitaria.it/macroeconomia-prospettiva-\
        europea-blanchard-olivier/libro/9788815265715'
1 fetch(link).then(function(response) {
        return response.text()
    }).then(function(html) {
2        var json = microdata.toJson(html, {
                    base: 'https://www.libreriauniversitaria.it'
                    });
3        var jsonld = microdata.toJsonld(html, {
                    base: 'https://www.libreriauniversitaria.it'
                  });
        console.log(JSON.stringify(json, null, 2));
        console.log(JSON.stringify(jsonld, null, 2));
    }).catch(function(err) {
        console.log('Failed to fetch page: ', err);
    });
```  

In maniera simile javascript scarica alla riga 1 il file html (in maniera asincrona), alla riga 2 avviene l'estrazione dei dati secondo il formato `microdata` mentre alla riga 3 in `jsonld`.
Il risultato sono due file JSON con i dati strutturati contenuti nella pagina rappresentati nei diversi formati. I dati estratti sono troppi per essere riportati in queste pagine e rimandiamo al sito allegato. Eccone solo un estratto:

```python
{
    "microdata": [
        {
            "type": "http://schema.org/WebPage",
            "id": "https://www.libreriauniversitaria.it/macroeconomia-\
                prospettiva-europea-blanchard-olivier/libro/9788815265715",
            "properties": {
                "breadcrumb": {},
                "mainEntity": {},
                        ......
            }
        }
    ]
    .....
  }
```

Per farci un’idea di come Google usi queste informazioni riportiamo un esempio tratto da [Google](https://developers.google.com/search/docs/guides/intro-structured-data#structured-data)  

```html
<script type="application/ld+json">
    {
      "@context": "https://schema.org/",
      "@type": "Recipe",
      "name": "Apple Pie by Grandma",
      "author": "Elaine Smith",
      "image": "http://images.edge-generalmills.com/56459281-6fe6-4d9d-984f-385c9488d824.jpg",
      "description": "A classic apple pie.",
      "aggregateRating": {
        "@type": "AggregateRating",
        "ratingValue": "4.8",
        "reviewCount": "7462",
        "bestRating": "5",
        "worstRating": "1"
      },
      "prepTime": "PT30M",
      "totalTime": "PT1H30M",
      "recipeYield": "8",
      "nutrition": {
        "@type": "NutritionInformation",
        "calories": "512 calories"
      },
      "recipeIngredient": [
        "1 box refrigerated pie crusts, softened as directed on box",
        "6 cups thinly sliced, peeled apples (6 medium)"
      ]
    }
    </script>
```

Questo script JSON (contenente dati in JSON-LD) viene utilizzato da Google nel modo mostrato nella seguente immagine.  

![Esempio tratto da Google Search](https://claudiotubertini.github.io/editoria/google_jsonld.png "Google JSON-LD")  

La presenza di dati strutturati nelle pagine web si è andata diffondendo e ora circa il 53% di tutti i siti ne usa uno o più tipi.
Questa è la distribuzione dell'uso di dati strutturati tratta da  [w3techs](https://w3techs.com/technologies/overview/structured_data/all)

![Statistica usi dati strutturati](https://claudiotubertini.github.io/editoria/structured_data_stats.png "Structured data stats")  

Come abbiamo visto anche nel nostro esempio, spesso vengono usati più formati nella stessa pagina.
In questi diversi formati il dato che cerchiamo ha la forma usuale di un oggetto con un certo valore che soddisfa un predicato, si parla quindi di “triplette”: oggetto, predicato, valore. Oggetto e predicato dovrebbero essere delle URI, il valore a volte non potrà che essere un numero o una stringa, ma se è a sua volta un’URI, tanto meglio.
Prendiamo l’RDFa riportato nella pagina precedente. L’oggetto è la nostra solita pagina web

```html
"@id": "https://www.libreriauniversitaria.it/macroeconomia-prospettiva-europea-blanchard-olivier/libro/9788815265715"
```

a cui si può applicare la relazione denominata ‘description’ (che i logici chiamano predicato)

```html
"http://ogp.me/ns#description"
```

che ha il valore:

```html
"@value": "Obiettivo del manuale - qui presentato in una nuova edizione radicalmente aggiornata anche nell'impianto teorico - è fornire una visione generale e integrata della macroeconomia post-crisi, adottando un modello di base che studia l'economia nel breve, nel medio e nel lungo periodo. Tale modello v ..."
```

Graficamente:
![RDFa](https://claudiotubertini.github.io/editoria/rdf_graph_new.png "RDFa")

Le relazione `http://ogp.me/#description` fa riferimento al vocabolario di entità e relazioni introdotte da OpenGraph Protocol `http://ogp.me/`, ma ve ne sono altre, altrettanto diffuse, ad esempio `https://schema.org`.

## 3. Contenuti interattivi

Abbiamo visto all'opera il web di documenti: come si possono comunicare dati e informazioni. Proveremo ora a guardare un po' più approfonditamente come si possono modificare i documenti in maniera dinamica, raccogliere e visualizzare dati, intesi come contenuti concettuali e non come dati "atomici" come abbiamo visto nella sezione precedente. Ora ci concentreremo sul lato utente (programmazione client-side), su quello che avviene nel browser subito prima, durante e subito dopo aver scaricato un file da un server web, termineremo la sezione con una rapida carrellata di come i contenuti si possano conservare efficacemente in strutture note come basi di dati.

### Visualizzazione client-side

Non ci sono in genere problemi a gestire testi e immagini in HTML, di cui avete visto qualche esempio nella sezione dedicata al web scraping, ma spesso vorremmo consentire al lettore un certo livello di interattività. Le tipiche azioni sono la ricerca, la scelta, la modifica e la memorizzazione degli elementi desiderati.Queste azioni possono essere svolte sia sul server remoto, che in remoto preparerà una nuova pagina e la invierà al lettore, ma spesso vendono svolte anche nel browser, e solo alcuni dati vengono salvati su un database. Questa programmazione, quando avviene sul server è fatta  in vari linguaggi (PHP, Python, Ruby, C# e NodeJS - JavaScript) ma è sempre fatta in javascript quando avviene sul client. Interattività vuol dire memorizzare i successivi stati in cui passa la pagina, mostrarli all'utente e reagire agli eventi (click, trascinamenti, selezioni, ecc.).  A questo proposito sono stati sviluppati dei frameworks che aiutano e semplificano il lavoro. I più diffusi sono [Vuejs](https://vuejs.org/),  [React](https://reactjs.org/), [Angular](https://angular.io/). Non avremo modo di trattare neppure di sfuggita questi strumenti, ma ci concentreremo invece su alcuni aspetti, la modifica del DOM, l'inserimento di contenuti nella pagina che sono comunque utili alla comprensione dei meccanismi con cui operano i frameworks client-side.
Ci occuperemo ora di uno dei temi più interessanti, sia da un punto di vista contenutistico-editoriale, sia da un punto di vista più squisitamente tecnico: la visualizzazione e rappresentazione dati utilizzando una delle più diffuse librerie, [D3](https://d3js.org/).
Il processo consiste sostanzialmente di tre fasi: il recupero dei dati, la loro modifica, selezione, integrazione secondo gli scopi che si è posto l'autore, la pubblicazione sulla pagina web, come semplice immagine, jpg o png, come pagina fissa, pdf, o meglio come elemento html.  La visualizzazione dati è argomento abbastanza vasto ma riportiamo qui una serie di esempi che possono servire come punti di [riferimento](https://github.com/wbkd/awesome-interactive-journalism). Noi ci limiteremo a qualche piccolo passo restando però all'interno della programmazione web

Il recupero dei dati, la gestione, selezione, il calcolo delle statistiche può essere fatto in Python o in R. Il risultato deve essere trasformato in html o passato a una libreria javascript per ottenere l'interattività sulla pagina web. Il modo in cui trattare i dati può essere suddiviso in una serie di passi come i seguenti (questo è un esempio scritto in R, i dati provengono da [mpg](https://ggplot2.tidyverse.org/reference/mpg.html), ecco un esempio di [esplorazione di dati](https://rpubs.com/shailesh/mpg-exploration). MPG è un database didattico che viene installato insieme a `ggplot`, che raccoglie i consumi di carburante di alcune marche automobilistiche)  

```R
ggplot(mpg, aes(displ, hwy)) + geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs(title = "Fuel efficiency generally
        decreases with engine size")
```

![Grafico](/home/claudio/Dropbox/corso_2019/manuale/mpg.png)

`ggplot` è la funzione che genera il grafico, `mpg` è l'oggetto, la struttura dati su cui si applicano le successive operazioni; vengono scelte le variabili `displ` (cilindrata, in litri) e `hwy` (galloni per miglio di autostrada), si applica poi una statistica, in questo caso una funzione polinomiale senza mostrare l'intervallo di confidenza (`se=FALSE`). Infine si aggiungono legenda, etichette ed altri elementi che aiutano la lettura.  
![Grafico a barre](/home/claudio/Dropbox/corso_2019/manuale/mpg_bar.png)  
_Consumi in funzione della marca automobilistica_

![Barre normalizzate](/home/claudio/Dropbox/corso_2019/manuale/mpg_bar_2.png)  
_Consumi normalizzati in funzione della marca automobilistca_

![Relazione fra consumi in città e su autostrada](/home/claudio/Dropbox/corso_2019/manuale/mpg_3.png)  
_Relazione fra i consumi nel traffico cittadino e in quello autostradale_

Questa procedura, "per strati"", è presente anche in D3 (e in molte altre librerie in vari linguaggi) e ha ricevuto anche un nome [The Grammar of Graphics](https://www.springer.com/gp/book/9780387245447), per una veloce introduzione leggete [A layered grammar of graphics](http://vita.had.co.nz/papers/layered-grammar.pdf) di Hadley Wickham.
Un approccio completamente diverso è disegnare direttamente nella pagina HTML attraverso l'elemento `<canvas>`. Per farsi un'idea un po' più compiuta si rimanda a [Canvas tutorial](https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial) .
Prendiamo in considerazione il file `canvas_example.html` che trovate nella pagina github, di cui riportiamo qui solo lo script:  

``` javascript
1 const canvas = document.querySelector('.myCanvas');
const width = canvas.width = window.innerWidth;
const height = canvas.height = window.innerHeight;
2 const ctx = canvas.getContext('2d');
3 ctx.fillStyle = 'rgb(0,0,0)';
ctx.fillRect(0,0,width,height);
ctx.fillStyle = 'rgb(255,0,0)';
ctx.fillRect(50,50,100,150);
ctx.fillStyle = 'rgb(0,255,0)';
ctx.fillRect(75,75,100,100);
ctx.fillStyle = 'rgba(255,0,255,0.75)';
ctx.fillRect(25,100,175,50);
ctx.strokeStyle = 'rgb(255,255,255)';
ctx.lineWidth = 5;
ctx.strokeRect(25,25,175,200);
```

L'elemento `<canvas>` è accessibile tramite un'API che consente di disegnare utilizzando javascript. Nella prima riga si accede al DOM individuando un elemento dotato della classe `myCanvas`, stabilite le dimensioni del canvas si costruisce (2) un oggetto, in questo caso a due dimensioni, potrebbe essere anche `webgl` (a tre dimensioni). Nelle righe successive vengono disegnati una serie di rettangoli.  Affinché l'immagine sia dinamica (appaia modificata) è necessario cancellare e ridisegnare l'intera immagine.  Si può anche inserire all'interno di canvas un'immagine svg generata con un editor come [Inkscape](https://inkscape.org/) o [Adobe Illustrator](https://www.adobe.com/products/illustrator.html).
Esiste una "terza via" ed è servirsi di [`D3`](https://d3js.org/) una libreria in grado di produrre rappresentazioni grafiche di ottimo livello, che si inseriscono facilmente nel DOM e che ben si prestano a una lettura interattiva. Ecco qualche [esempio](https://observablehq.com/@d3/gallery).
Vediamo nelle linee generali come si procede.
Il primo passo è recuperare i dati che verranno rappresentati nella pagina web. Per farlo in javascript leggete l'ottimo [Learn JS Data](https://observablehq.com/@dakoop/learn-js-data) che illustra i concetti base dell'analisi dei dati, analisi che in Python viene fatta con [pandas](https://pandas.pydata.org/docs/getting_started/intro_tutorials/), in `R` con [tidyverse](https://www.tidyverse.org/packages/).
Nel file `index_1.html` trovate il primo passaggio, il caricamento dei dati.  

```html
<script>
  function test(){
  d3.csv("https://claudiotubertini.github.io/WHOmanities/covid_italy.csv")
      .then(data => data.map(x => {return {
        day: x["day"],
        Region: x["Region"],
        country: x["Country Name"],
        Confirmed: +x["Confirmed"],
        "Cumulative Confirmed": +x["Cumulative Confirmed"]}}))
      .then(function(data){return console.log(JSON.stringify(data, null, 2))});
}
</script>

</head>

<body onload="test()">
```

`D3` ha funzioni che caricano file in diversi formati. Ho utilizzato i dati della pandemia di Covid prelevati dal sito [European Centre for Disease Prevention and Control](https://data.europa.eu/euodp/en/data/dataset/covid-19-coronavirus-data) dove è disponibile pubblicamente un file csv con i dati suddivisi per nazione. Sul sito dedicagto a questo volume trovate i dati di Italia e Germania dal 28 gennaio al 15 maggio 2020. La funzione `d3.csv(url)` ha le caratteristica di una funzione asincrona, ricevuta la risposta procede a scaricare solo i dati indicati, `data.map(x => { return {}})` applica a ogni riga del file csv la funzione che vedete, sceglie cioè solo alcuni campi, gli altri non sono necessari. Oltre alla data, la nazione (scaricheremo sia Italia che Germania, quindi ci serve), scarica sia i "confermati", cioè il numero di ammalati di un certo giorno, che il valore cumulato. Infine, terminata questa operazione, stampa nella console il risultato producendo anche un'impaginazione leggibile.
Ritornando all'idea di grammatica a strati guardiamo ora come possiamo inserire i dati negli elementi della pagina html, vedremo poi come rappresentarli graficamente, come formulare tutto il necessario: assi, linee, punti, etichette, ecc.
Come possiamo modificare la nostra pagina?  

```html
<!DOCTYPE html>
<html lang="it">
  <head>
    <title>D3: Selezioni</title>
  </head>
  <body>
    <div class="about-me">
      <p>Ciao benvenuti al Master in comunicazione storica</p>
      <ul id="list">
        <li>Sono dotato di superpoteri</li>
        <li>Sto studiando editoria digitale</li>
        <li>Adoro D3.js!</li>
      </ul>
    </div>

    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="main.js"></script>
  </body>
</html>
```

In genere per ottenere una certa interattività si aggiungono degli [eventi](https://developer.mozilla.org/en-US/docs/Web/API/Event), prima abbiamo visto `onload`, in questo caso potremmo usare `onclick`. Nel file non è presente nessun evento quindi la modifica al DOM avverrà nel momento in cui si carica il file `main.js` che contiene appunto le modifiche.  

```javascript
1 const listItems = [...document.getElementsByTagName('li')];
2 listItems.map(item => item.style.setProperty('font-weight', 'bold'));
const ul = document.getElementsByTagName('ul')[0];
const newItem = document.createElement('li');
newItem.appendChild(document.createTextNode(`Cerco di capire le selezioni in D3.`));
ul.appendChild(newItem);
```

Iniziamo da questo script che non contiene nessun comando D3 ma semplice javascript, quello che spesso viene chiamato _vanilla javascript_. La prima riga contiene `document.getElementsByTagName('li')` che recupera tutti i `li`, l'operatore 'spread' li passa direttamente come una lista alla variabile `listItems`. Ora tocca alla funzione `.map` che ad ogni elemento della lista a cui viene applicata (ora `listItems.map()`) applica la funzione `item => item.style.setProperty('font-weight', 'bold')`. Comando che trasforma lo stile da normale in grassetto. Lo stesso comando poteva essere gestito da CSS ma avremmo avuto comunque il problema di "quando" applicare la trasformazione.  Ritorniamo al nostro esempio. I tag `<li>` devono essere all'interno di un `<ul>`; recuperiamo quello che vogliamo modificare, `document.getElementsByTagName('ul')[0]`; infine creaiamo un nuovo `<li>`, vi aggiungiamo del testo e lo aggiungiamo al tag `<ul>`. Si tratta di un'operazione di _routine_ che richiede però un certo sforzo. Nelle normali applicazioni web dove le modifiche, aggiunte, aggiornamenti possono essere in numero importante un'operazione di questo genere costituisce un problema. Per risolvere simili difficoltà sono stati sviluppati vari frameworks (come i già citati 'Vue' e 'React') che accelerano e semplificano questo lavoro.
Vediamo come si può procedere usando `D3`, uno strumento specializzato nella visualizzazione dati, 'Vue' e 'React' sono generali e possono essere usati per costruire integralmente un'applicazione web.  

```javascript
const li = d3.selectAll('li').style('font-weight', 'bold');
const ul = d3.select('ul');
ul.append('li').text(`I'm learning about selections`);
const ul = d3.select('ul');
ul.select('li').style('color', 'red');
ul.append('li').text(`I'm learning about selections`);
```

Le operazioni da fare sono solo `select` o `selectAll` (nel caso vi siano una sequenza di elementi). La selezione in `D3` individua un elemento, se l'elemento non esiste viene creato, posto che vi si inseriscano dati o assegnati altri elementi, in questo caso viene aggiunto del testo in grassetto.

`d3.select()`, seleziona il primo elemento per nome, classe, id o per gli altri elementi CSS;
`d3.selectAll()` seleziona un array, sempre attraverso iselettori CSS.
Fatta la selezione gli elementi possono essere modificati, ad esempio:
`d3.selectAll(“circle”).attr(“fill”, “#ff0000”)` qui 'circle' è un comando SVG,
`d3.select(“h1”).text(“Hello World!”)`
`d3.select(“svg”).append(“rect”).attr(“x”,50).attr(“y”,100).attr(“width”,100).attr(“height”,100).attr(“fill”,”#0000FF”)` viene creato un rettangolo, aggiunto a `<svg>`, nel punto di coordinate (50, 100), di base 100 e altezza 1000.
Colleghiamo i dati agli elementi selezionati con la funzione data()

```javascript
var svg = d3.select("body").append("svg") // all'elemento <body> aggiungiamo <svg>
svg.selectAll()  // generiamo una serie di elementi, per ora indeterminati
    .data(data) // li colleghiamo ai dati che abbiamo recuperato precedentemente
    .join("rect")  // i dati vengono collegati a rettangoli, un dato per ogni rettangolo
    .attr("class", "bar")
    .style("fill", 'lime')
```

La caratteristica di `.join()` è di sincronizzare i dati con gli elementi selezionati del DOM. Se vi sono più dati che elementi provvede a crearne di nuovi se invece i dati sono in nnumero maggiore degli elementi elemina gli elementi di troppo. Ovviamente lo sviluppatore può controllare queste operazioni con `.join().enter()`, `.join().update()` o `.join().exit()`. Si veda [Mike Bostock, selection.join, 2019](https://observablehq.com/@d3/selection-join).
Ricapitoliamo: abbiamo scaricato i dati, siamo riusciti a collegarli ciascuno a un rettangolo che costituirà il grafico a barre, ora dovremo aggiungere gli elementi grafici generali, vedi il file `index_2.html`:

```javascript
// stabiliamo le dimensione del grafico e il margine, che è lo spazio tra l'elemento e il bordo della "finestra"
var margin = {top: 40, right: 20, bottom: 70, left: 100};
width = 1260 - margin.left - margin.right;
height = 700 - margin.top - margin.bottom;
// stabiliamo le estensioni dell'asse delle ascisse e delle ordinate, i "ticks" sono quelle piccole linee che aiutano il lettore a seguire l'andamento
// dell'ordinata
var x = d3.scaleBand().range([0, width]);
var y = d3.scaleLinear().range([height, 0]);
var xAxis = d3.axisBottom().scale(x);
var yAxis = d3.axisLeft().scale(y).ticks(10);
```

Il nostro obiettivo è mettere a confronto i dati covid in Italia con quelli in Germania. Scaricheremo allora entrambi gli insiemi di dati, costruiremo un grafico, aggiungeremo un evento 'onclick' e traformeremo il grafico modificando i dati. Per vedere il risultato guardate il file `index_3.html`. Fate attenzione, dopoa ver cliccato sul grafico dovrete aspettare qualche secondo perché le prime modificazioni riguardano giorni senza pazienti e non sono quindi visibili, ma lo script effettua comunuque i calcoli.  

```javascript
svg.on( "click", function() {
  [ data, data2 ] = [ data2, data ];
  mycolor = (mycolor == "steelblue")? "lime" : "steelblue";
svg.selectAll(".bar").data(data)
  .transition().duration( 1000 ).delay( (d,i)=>100*i )
  .style("fill", mycolor)
  .attr("x", function(d) { return x(d.date); })
  .attr("width", 10) 
  .attr("y", function(d) { return y(d["Cumulative Confirmed"]); }) 
  .attr("height", function(d) { return height - y(d["Cumulative Confirmed"]); });
  });
```

Una volta cliccato sul grafico, la funzione "callback", detta così proprio perché viene eseguita come risposta, sostituisce i dati dell'Italia con quelli della Germania (1) e ne modifica il colore, vengono poi selezionati i rettangoli a cui vengono passati i nuovi dati, effettuata una transizione con un certo ritardo, ogni rettangolo si modifica 0.1 secondi dopo il precedente, un intervallo sufficiente a dare un'impressione di "morbido" cambiamento.  

#### ePub e interattività

Questi tipi di rappresentazioni grafiche, sia fatte con `canvas` che con `D3` possono essere usate all'interno di un ePub 3.0. I file html con javascript possono essere inseriti all'interno di un `iframe` così da lasciare inalterate le altre parti dell'ePub, sia nel caso il lettore non contmpli l'uso di kavascript, sia nel caso vi sia un uso improprio dello script. La possibilità che i programmi in javascript possano accedere all'esterno del browser in cui vengono utilizzati è rigidamente controllato, anche in questo caso l'obbligo di limitarsi a un `iframe` ha la stessa funzione. Per un esempio leggi [EPUB Content Documents 3.2, 2019](https://www.w3.org/publishing/epub32/epub-contentdocs.html#example-4)

### Conservare i dati (DA FARE)

Fino ad ora abbiamo sempre parlato di programmi eseguibili in una console o in un browser, programmi che producono dati che spariscono definitivamente una volta che la finestra del browser o la console vengono chiusi. Se usate node potete accedere al file system del vostro computer e salvare i dati in un file.
Da un browser vedi [Mozilla](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Client-side_web_APIs/Client-side_storage)
Altrimenti sui un server [Mozilla](https://developer.mozilla.org/en-US/docs/Learn/Server-side/First_steps)

## 4. Tecnologie XML

Per poter trasmettere, comunicare un testo, potete provare visualizzando in un browser

```html
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
</head>
<!-- Quello che segue è un titolo, h1, con un semplice testo -->
<body>
<h1>Hello world!</h1>
</body>
</html>
```

ma non è una soluzione particolarmente comoda: la dimensione dello schermo su cui guardiamo, il tipo di browser e altre caratteristiche che vedremo nelle pagine che seguono, non fanno di questo file uno strumento adeguato al nostro obiettivo: che è inviare un messaggio che conservi le dimensioni fisse che siamo abituati ad avere sulla carta, distanze e rapporti con le immagini e i bordi.
Proviamo ora con un file PDF generato direttamente dalla nostra pagina, facendo uso di una library javascript [jsPDF](https://github.com/MrRio/jsPDF) che non abbiamo sul nostro computer ma che scarichiamo "al volo" dal web (si veda il file `pdf.html`):  

```html
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jspdf/1.5.3/jspdf.debug.js"></script>
</head>
<script> 
// La dimensione pagina di default è l'a4, portrait
 var doc = new jsPDF()
doc.text("Hello world!", 100, 40, angle=5) // i numeri sono le coordiante sulla pagina in pixel dell'angolo in alto a sinistra, l'angolo è in gradi, in senso antiorario
 img.src = 'world_local.png';
doc.addImage(img, 'png', 10, 50, 30, 20);
doc.save("a4.pdf")
</script>
</html>
```  

Potete controllare facilmente i comandi chiave nella [documentazione](http://raw.githack.com/MrRio/jsPDF/master/docs/) di jsPDF. La cosa interessante è tutta in `doc.text("Hello world!", 100, 40, angle=5)`. L'oggetto `doc` riceve le coordinate dove apparire sulla pagina, e un orientamento in gradi. La riga successiva presenta l'indirizzo del file da caricare. Torneremo varie volte su questa caratteristica di Javascript ma per ragioni di sicurezza, questo linguaggio, quando è all'interno di un browser, non può accedere al sistema operativo. È necessario allora che i file di questa cartella siano gestiti da un server web così che il browser possa accedere ad essi con un'usuale connessione http.  Chi conosce nodejs può lanciare, da dentro la cartella in cui si trova `pdf.html`

```bash
npm install -g http-server
http-server -p 8080 --cors
```

oppure con python `python -m http.server 8080`. Il file andrà quindi visualizzato all'indirizzo `localhost:8080/pdf.html`
Con il comando `doc.addImage(img, 'png', 10, 50);` viene aggiunta l'immagine nella posizione indicata dalle coordinate. Infine `save` inizia il download del file, controllato dal browser.
Il file PDF consiste in una serie di pagine che riportano ciascuna tutte le informazioni necessarie a riprodurre il testo, le immagini, i fonts, le distanze relative, i metadati, ma anche la progressione lineare delle pagine, così che un "reader" in grado di decodificare queste informazioni può ricostruire il documento, così come fosse stampato da una "stampante virtuale".

L'esempio di PDF prodotto con la library `jsPDF` è adatto alla produzione di un file all'interno di un'applicazione, ad esempio la fattura di acquisto in un sito di ecommerce, il report delle statistiche di vendita di un gestionale aziendale o anche il testo di un blog in un formato facilmente conservabile e leggibile _offline_.
Nella produzione editoriale il più delle volte si parte da un file di testo consegnato da un autore e non da un file html, ma il nostro esempio ci dà comunque un'idea della procedura necessaria per arrivare a un output in PDF. Gli strumenti più diffusi sono

- i software di impaginazione, i più noti (commerciali) sono [Adobe InDesign](https://www.adobe.com/products/indesign.html?promoid=KLYUU&s_cid=70114000002CfGJAA0&s_iid=70114000002ChdJAAS) e [Quark Xpress](http://www.quark.com/Products/QuarkXPress/), che sono pensati per essere usati da un grafico che riceve i testi dall'autore;
- Latex (e relative varianti), open source, destinato agli autori, ma anche ai grafici/programmatori che rielaborano il testo di altri. In assoluto il migliore in ambito scientifico o quando l'impaginazione è complessa, come nelle edizioni critiche di testi letterari; per approfondire questo fondamentale strumento per la produzione personale dei vostri scritti potete consultare i tutorial presenti su [Overleaf](https://www.overleaf.com/learn/latex/Free_online_introduction_to_LaTeX_(part_1)), un interessante editor online. In alternativa si può leggere utilmente [LATEX](https://en.wikibooks.org/wiki/LaTeX). Esiste anche una comunità importante di sviluppatori italiani [GuIT](https://www.guitex.org/home/).
- infine bisogna ricordare tutte le diverse tecnologie legate alla manipolazione automatica dei diversi linguaggi di markup. Partendo da un file XML, esistono soluzioni per trasformare in XML testo proveniente persino da Microsoft Word, ad esempio lo splendido [pandoc](https://pandoc.org/) e in genere gli editor XML come [Oxygenxml](https://www.oxygenxml.com/) hanno funzionalità con questo scopo, il testo può essere formattato in PDF attraverso fogli di stile XSL-FO, e recentemente anche CSS. Esistono anche prodotti commerciali [Antenna House](http://www.antennahouse.com/), [Prince](http://www.princexml.com/) destinati all'uso industriale.

### XML e linguaggi di markup

Tra le varie modalità di preparazione di un file PDF accenneremo solo al linguaggio XSLT, e alla sua variante XSL-FO. Non parleremo invece né di Latex, che vi invito a studiare partendo dalla bibliografia indicata (ne vale la pena), né tantomeno dei software commerciali di impaginazione. Va detto a questo punto che il processo di produzione nelle aziende editoriali è in larga parte monopolizzato da InDesign e QuarkXpress, con una piccola parte di lavori gestiti in XML (attraverso [Framemaker](https://www.adobe.com/products/framemaker.html)). Noi accenneremo solo alla produzione di PDF mediante strumenti "programmatici".
L'esempio più semplice di linguaggio di markup è l'HTML. Un linguaggio, discendente in un qualche modo dall'XML, che raccoglie al proprio interno sia tag sintattici (`<div>` o `<span>`) sia tag semantici (`table`, `h1`, `<header>`, `<footer>`, ecc.). I browser sono in grado di correggere molti errori della sintassi HTML. Anche se ci siamo dimenticati di chiudere un tag la nostra pagina verrà comunque visualizzata, sempre che il browser riesca a indovinare cosa abbiamo dimenticato, ma succede spesso.
Nella trasmissione di dati, di file, in generale di contenuti all'interno di un'applicazione dobbiamo ricorrere a una sintassi e a una semantica più rigorose. I file non sono scritti "a mano" ma sono il risultato di una funzione e se manca un tag di chiusura o è un errore, e va segnalato, o è una scelta deliberata. L'XML è il linguaggio (una collezione di norme con annesso un vocabolario, che a volte viene chiamata _ontologia_) utilizzato nella trasmissione dei contenuti. Parleremo anche del suo principale concorrente, [JSON](https://www.json.org/json-en.html) un formato pensato per lo scambio di dati, nei capitoli successivi. Va detto che nelle applicazioni editoriali (e altre nel mondo dell'industria) l'XML mantiene ancora un ruolo importante, è più complesso da produrre e manipolare rispetto a un JSON, ma il parsing (ovvero la lettura) di un XML è viceversa più semplice, e tanto più semplice quanto i dati diventano più complicati.

### Prepariamo un file XML  

Prendiamo ad esempio il file `Bookstore-DTD.xml` che trovate nella pagina di github collegata a questo saggio. (Parte del materiale qui utilizzato proviene da [Stanford's Databases MOOC, Jennifer Widom, 2012](https://cs.stanford.edu/people/widom/DB-mooc.html) che oggi, giugno 2020, non mi risulta pubblicamente disponibile) I tag sono organizzati in maniera gerarchica in questo modo:
`<Bookstore>` è l'elemento radice che può contenere un numero imprecisato di `<Book>` o di `<Magazine>`. A sua volta `<Book>` (ma anche `<Magazine>`) conterrà un `<Title>`, uno o più `<Author>`, ecc. Un elemento come `<Book ISBN="ISBN-0-13-713526-2" Price="85" Edition="3rd">` contiene anche degli attributi (stringhe di nome/valore separate da spazi). La sintassi esatta degli elementi è contenuta nella parte iniziale del file, in quella che si chiama DTD. Spesso si usa separare le definizioni del vocabolario di tag  in un file di tipo `xsd` (schema XML). In linea di principio ognuno potrebbe costruirsi il proprio vocabolario per ciascun argomento di cui intende occuparsi. Per fortuna si sono consolidati alcuni vocabolari specifici, ad esempio DocBook (per l'editoria in generale) [DocBook](https://github.com/docbook/wiki/wiki), [DocBook XSL – The Complete Guide](http://www.sagehill.net/docbookxsl/) e [TEI](https://tei-c.org/) per le discipline umanistiche in particolare.
Vediamo ora qualche caso concreto (nella pratica quotidiana raramente si lavora direttamente sui file come faremo ora noi, per ragioni didattiche. Vengono usati sempre editor come quelli che trovate in [questa pagina](https://help.ubuntu.com/community/DocBookEditors)).
Per poter lavorare con i file XML utilizzeremo `xsltproc` un programma che legge file XSLT, file che contengono una serie di trasformazioni che vogliamo applicare al file XML di input, regole che generano un file di testo di output, che potrà essere a sua volta XML, HTML, csv o semplice txt. Tutti questi file non sono altro che file di testo dotati di sintassi diverse così che a loro volta potranno essere usati per ulteriori lavorazioni. Vedremo qualche esempio di XSLT, uno strumento in grado di modificare profondametne la struttura del testo a cui viene applicato. Installeremo poi `docbook-xls`, una serie di file XSLT che gestisono le trasformazioni secondo il vocabolario docbook. Potremo modificare titoli, sottotitoli, sezioni ecc. utilizzando questo standard,  diffuso in molte produzioni editoriali già da molti anni. Infine installeremo `apache fop`, che ci consentirà di produrre file pdf dai file in docbook. Vedremo anche qualche esempio con pandoc.
Tutte le installazioni che vedremo in questo saggio hanno come riferimento il sistema operativo linux, in particolare ubuntu/debian. Chi dispone di Mac o Windows troverà facilmente nella bibliografia le indicazioni adatte al proprio caso.
L'installazione in ubuntu/debian è la seguente

```bash
sudo apt update
sudo apt install docbook-xsl
sudo apt install xsltproc
sudo apt install fop
sudo apt docbook-xsl-doc-pdf
```

_DA FARE_ Provare con Saxon-JS al posto di xsltproc.

I file e i comandi che seguono richiedono solo queste installazioni di default, se volete utilizzare Apache fop più aggiornato potete leggere [fop-2.5](http://www.linuxfromscratch.org/blfs/view/svn/pst/fop.html).
Ritorniamo a `Bookstore-DTD.xml`

```XML
<Bookstore>
   <Book ISBN="ISBN-0-13-713526-2" Price="85" Edition="3rd">
      <Title>A First Course in Database Systems</Title>
      <Authors>
         <Author>
            <First_Name>Jeffrey</First_Name>
            <Last_Name>Ullman</Last_Name>

         </Author>
         <Author>
            <First_Name>Jennifer</First_Name>
            <Last_Name>Widom</Last_Name>
         </Author>
      </Authors>
      <Remark></Remark>
   </Book>
   <Book ISBN="ISBN-0-13-815504-6" Price="100" Edition="">
      <Title>Database Systems: The Complete Book</Title>
      <Authors>
         <Author>
            <First_Name>Hector</First_Name>
            <Last_Name>Garcia-Molina</Last_Name>
         </Author>
         <Author>
            <First_Name>Jeffrey</First_Name>
            <Last_Name>Ullman</Last_Name>
         </Author>
         <Author>
            <First_Name>Jennifer</First_Name>
            <Last_Name>Widom</Last_Name>
         </Author>
      </Authors>
      <Remark>
         Buy this book bundled with <BookRef book="ISBN-0-13-713526-2"/>a great deal!
      </Remark>
   </Book>
</Bookstore>
```  

Direi che i tag sono autoesplicativi. Se il file fosse fatto di alcune migliaia di righe dovreste ricorrere o alle istruzioni di chi ha preparato il file, allo schema del file, DTD o XSD, o come più comunemente succede, studiare lo schema per tentativi. Gli editor commerciali com Oxygenxml fanno l'analisi in maniera automatica al vostro posto. Lo studio dovrà essere fatto mediante comandi XPath di cui abbiamo parlato nella precedente sezione. Per ora accontentiamoci di un `<Bookstore>` con alcuni `<book>` dotati di `<Title>` e di `<Authors>`. A volte potrebbero esserci dei `<Remark/>`, ma nel nostro caso l'elemento è vuoto.
Per manipolare questo file possiamo usare una trasformazione XSLT. In generale si indica con XSL una famiglia di standards per la trasformazione e presentazione di file XML che consiste di tre parti:

1. XSL Transformations (XSLT), un linguaggio per la trasformazione dell'XML, di cui subito vedremo qualche esempio.
2. XPath, che consente di "attraversare" i file XML, individuando singoli elementi o parti di XML. L'XPath viene usato non solo all'interno di XSLT, ma anche in altri linguaggi di programmazione vista l'efficienza della ricerca.
3. XSL Formatting Objects (XSL-FO), un vocabolario XML per la fomattazione dei file, di fatto utilizzato per la produzione di PDF. Per la preparazione di file HTML o altri formati testo basta utilizzare XSLT. Per ottenere un PDF invece è necessario prima applicare una trasformazione con XSL-FO e poi da questo, con un altro processore arrivare al file PDF.  

```XSLT
<xsl:stylesheet version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform">
<xsl:output method="xml" indent="yes" omit-xml-declaration="yes" />

<xsl:template match="Book">
  <BookTitle> <xsl:value-of select="Title" /> </BookTitle>
</xsl:template>

<xsl:template match="Magazine">
  <MagazineTitle> <xsl:value-of select="Title" /> </MagazineTitle>
</xsl:template>

</xsl:stylesheet>
```

La prima riga specifica la versione del foglio di stile XSLT, Docbook usa la versione 1.0, ma il W3C ha pubblicato la [3.0](https://www.w3.org/TR/2017/REC-xslt-30-20170608/): le successive versioni contengono ad esempio la ricerca con espressioni regolari, un uso più sofisticato di variabili, ma per gli scopi che Docbook si pone la 1.0 è sufficiente (una versione 2.0 è da tempo in lavorazione ma non copre ancora tutti gli aspetti di DocBook).
La seconda riga specifica il tipo di output, in questo caso "xml", ma poteva essere anche html o text.

```bash
xsltproc -o output.xml bookstore_1.xsl Bookstore-DTD.xml
```

il risultato è il seguente:

```xml
  <BookTitle>A First Course in Database Systems</BookTitle>
  <BookTitle>Database Systems: The Complete Book</BookTitle>
```

Il primo template cerca (`match="Book"`) l'elemento `Book` (questo è un XPath, anche se non lo sembra), una volta trovato applica la trasformazione `<BookTitle> <xsl:value-of select="Title" /> </BookTitle>` che consiste nel mettere il valore selezionato dall'elemento `Title` (di nuovo un XPath) all'interno del tag `<BookTitle>`.
Questo  piccolo esempio consente già di intuire le grandi potenzialità di intervento sul testo possedute dalle trasformazioni XSLT. Nel caso l'output fosse stato html, avremmo dovuto inserire i tag per generare un html corretto, se avessimo voluto un file csv avremmo dovuto inserire le virgole e i nomi dei campi. L'XSLT non ha nulla di magico ma avendo la capacità di modificare efficacemente il testo, trovato mediante l'XPath, dà risultati che in altro modo, usando espressioni regolari o altre strutture ripetitive, non sarebbero ottenibili. Questa efficienza è dovuta all'uso del markup.
Proviamo ora questa trasformazione: scegliamo i titoli che abbiano prezzo inferiore a 90 euro.

```xslt
<xsl:template match="Book[@Price &lt; 90]">
   <xsl:copy-of select="." />
</xsl:template>

<xsl:template match="text()" />
```

Ora l'XPath è leggermente più complesso: `Book[@Price &lt; 90]`. Lo possiamo tradurre in: trova tutti gli elementi `Book` che soddisfino la regola indicata fra parentesi `[ ]`, in questo caso l'attributo `Price` deve essere inferiore a 90, `@` indica che `Price` è un attributo di `Book`. "Inferiore" deve essere indicato, per non confondersi con i segni di inizio o fine tag, con `&lt;` invece che `<`. Oltre al primo template è necessario aggiungerne un secondo che elimina il testo che passa indenne al controllo del primo template. Questo template, se trova del testo si limita a non farne nulla, quindi quel testo non comparirà nell'output. Trovate il file `bookstore_2.xsl` nel sito github.

```bash
xsltproc -o output.xml bookstore_2.xsl Bookstore-DTD.xml
```

e avrete come risultato la copia completa, grazie a `<xsl:copy-of select="." />`, dell'elemento `Book` con `Price` minore di 90
Vediamo ora un esempio un po' più articolato.

```xslt
<xsl:stylesheet version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform">
<xsl:output method="html" indent="yes" />

<xsl:template match="/">
   <html>
     <head/>
   <table border="1">
      <th>Book</th>
      <th>Cost</th>
      <xsl:for-each select="Bookstore/Book">
      <xsl:sort select="@Price" />
         <xsl:if test="@Price &lt; 90">
            <tr>
            <td><i><xsl:value-of select="Title" /></i></td>
            <td><xsl:value-of select="@Price" /></td>
            </tr>
         </xsl:if>
      </xsl:for-each>
   </table>
   </html>
</xsl:template>

</xsl:stylesheet>
```

Questo programma trasforma il nostro file XML in un file HTML. Parte dall'elemento `root` (`match="/"`), inserisce il corretto codice HTML, cerca tutti i `Book`, li ordina in base al prezzo e se costano meno di 90 ne riporta il titolo e il prezzo.

```bash
xsltproc -o output.html bookstore_3.xsl Bookstore-DTD.xml
```

e questo è il risultato

```html
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head>
<table border="1">
<th>Book</th>
<th>Cost</th>
<tr>
<td><i>A First Course in Database Systems</i></td>
<td>85</td>
</tr>
</table>
</html>
```

### Trasformare testo in PDF

Ecco un esempio di docbook.  

```xml  
<!-- file manuale_1.xml-->
<article>
    <info>
        <title>Capitolo di prova</title>
        <author>
            <personname><firstname>Mario</firstname><surname>Rossi</surname></personname>
        </author>
    </info>
    <sect1>
        <title>Evviva l'XML</title>
        <para>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
            incididunt ut labore et dolore magna aliqua.</para>
        <para>Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi
            ut aliquip ex ea commodo consequat. </para>
        <itemizedlist>
            <listitem>
                <para>Duis aute irure dolor in reprehenderit </para>
            </listitem>
            <listitem>
                <para>in voluptate velit esse cillum dolore eu fugiat nulla pariatur. </para>
            </listitem>
        </itemizedlist>
    </sect1>
</article>
```

Ho tralasciato le dichiarazioni XML e i "namespaces" che individuano il vocabolario limitandomi a riportare i tag che ci aiutano a comprendere meglio lo script. Potete controllare su github il file completo il cui nome è riportato nella prima riga.
In Docbook sono presenti alcune centinaia di tag che riescono a coprire quasi tutti i casi possibili che si incontrano in una pubblicazione. Compreso un sottoinsieme noto come [DocBook Publishers](https://tdg.docbook.org/tdg/publishers/5.0/index.html) che presenta solo i tag usati nell'editoria libraria.
Coome esempio riportiamo alcuni tag presenti solo in Docbook Publishers:

- dcterms: In Docbook Publishers sono stati inseriti molti termini della collezione di metadati nota come [Dublin Core](https://dublincore.org/)
- dialogue — Il contenitore per i dialoghi fra personaggi teatrali (ad esempio)
- drama — Il contenitore di dialoghi per opere teatrali
- line — Una riga di dialogo o un verso di poesia
- linegroup — Un gruppo di righe di dialoghi o versi di poesia

```bash
xsltproc -o test.html /usr/share/xml/docbook/stylesheet/nwalsh/html/docbook.xsl manuale_1.xml
```

lanciando questo comando da console si ottiene un file html. Per avere un PDF è necessario invece generare prima un file con XSL-FO e poi con un processore come Apache fop generare il vero e proprio PDF.

```bash
xsltproc -o intermediate-fo-file.fo /usr/share/xml/docbook/stylesheet/nwalsh/fo/docbook.xsl manuale_1.xml
fop -pdf final-pdf-file.pdf -fo intermediate-fo-file.fo
```

Come vedete sono comandi sostanzialmente ripetitivi. Si suol dire che i programmatori sono dei pigri perché vogliono automatizzare tutti i processi ripetitivi. I diversi comandi si possono raccogliere e far eseguire a un file denominato [`makefile`](http://www.sagehill.net/docbookxsl/Makefiles.html). Il linguaggio [`Make`](https://www.gnu.org/software/make/manual/make.html) è abbastanza sofisticato, noi ci limiteremo a un esempio molto semplice:

```makefile
html:
xsltproc -o test.html /usr/share/xml/docbook/stylesheet/nwalsh/html/docbook.xsl manuale_1.xml

pdf:
xsltproc -o intermediate-fo-file.fo /usr/share/xml/docbook/stylesheet/nwalsh/fo/docbook.xsl manuale_1.xml
fop -pdf final-pdf-file.pdf -fo intermediate-fo-file.fo
```

Usando questo `makefile` i comandi diventano, nella stessa directory  `make html` oppure per il pdf `make pdf`. Se i comandi inseriti nel `makefile` sono molti il vantaggio è importante, soprattutto in un ambiente come quello editoriale dove la correzione bozze, ad esempio, richiede innumerevoli modifiche al testo e un costante controllo del risultato finale.
In questi esempi abbiamo usato Apache Fop ma esistono molti altri processori. Li potete trovare in [questa pagina](https://www.w3.org/Style/XSL/).
L'alternativa che ritengo più semplice è l'utilizzo di Latex attraverso pandoc.
Oppure con [pandoc](https://pandoc.org/):

```bash
saxon -o docbook-file.xml custom.xml stylesheet.xslt #generate DocBook
pandoc -f docbook docbook-file.xml -t context --standalone --template template.tex -o out.tex #generate ConTeXt
context out.tex #generate PDF -->

pandoc --from docbook --to latex --output myDocbook.pdf manuale_1.xml
```  

## Conclusioni  

In queste pagine abbiamo visto esempi di file html e xml in diverse situazioni, più o meno riconducibili ad attività web rilevanti per l'editoria. E questo è vero anche per il lavoro con DocBook, anche se gli esempi facevano solo riferimento ad applicazioni desktop. C'è un convitato di pietra in questa pagine, forse intravisto solo nella seconda sezione: lo sviluppo di API (Application programming Interface), naturale sviluppo delle nozioni qui discusse e porta di accesso al _cloud_ dove le nozioni di programmazione lato utente, lato server e desktop perdono di significato e dove i contenuti assumono il ruolo centrale che meritano.
